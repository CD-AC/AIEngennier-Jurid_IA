{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPkA/uVtuTGdPQiXsIBrcLl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CD-AC/AIEngennier-Jurid_IA/blob/main/Jurid_IA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Jurid-IA"
      ],
      "metadata": {
        "id": "urxe0XE_uz1T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "un asistente de inteligencia artificial experto diseñado para actuar como un único punto de acceso inteligente a todo el conocimiento de la organización. La solución se fundamenta en una arquitectura moderna de RAG (Retrieval-Augmented Generation), orquestada con el framework LangChain. Para automatizar la ingesta de datos, se implementó un pipeline ETL robusto con Apache Airflow, que extrae, transforma y procesa documentos de las distintas fuentes."
      ],
      "metadata": {
        "id": "PI7UoAw3u3gu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalar Dependencias"
      ],
      "metadata": {
        "id": "glbOXLS7u59x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GAGnxQ7O1mxU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f88ba22-b92d-412f-d76d-7abbf3eec91d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/74.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.5/443.5 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.6/587.6 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.1/221.1 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain_openai langchain_community gradio openai python-dotenv beautifulsoup4 pinecone-client langchain-pinecone"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importar Dependencias y Configuración Inicial"
      ],
      "metadata": {
        "id": "F9gxzzK2vKG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import time\n",
        "import gradio as gr\n",
        "from pinecone import Pinecone as PineconeClient, ServerlessSpec\n",
        "\n",
        "# LangChain Imports\n",
        "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_pinecone import Pinecone\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "6s4ZZYWc11Gp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuración OpenAI - Pinecone"
      ],
      "metadata": {
        "id": "jhlnK1EZvcOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    # OpenAI\n",
        "    LLM_MODEL_NAME = \"gpt-4o-mini\"\n",
        "    EMBEDDING_MODEL_NAME = \"text-embedding-3-small\"\n",
        "    EMBEDDING_DIMENSION = 1536\n",
        "\n",
        "    # Directorio\n",
        "    KNOWLEDGE_BASE_DIR = \"/content/knowledge-base\"\n",
        "\n",
        "    # Pinecone\n",
        "    PINECONE_INDEX_NAME = \"jurid-ia-idx\"\n",
        "\n",
        "    # Parámetros text spliter\n",
        "    CHUNK_SIZE = 1000\n",
        "    CHUNK_OVERLAP = 150\n",
        "\n",
        "    # Parámetros de Búsqueda del Retriever\n",
        "    SEARCH_K = 5"
      ],
      "metadata": {
        "id": "KWLtBs6YgJsi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuración de API_KEY"
      ],
      "metadata": {
        "id": "L5FN-vL5v5na"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar API Key de OpenAI\n",
        "openai_api_key_file = '/content/OPENAI_API_KEY.txt'\n",
        "try:\n",
        "    with open(openai_api_key_file, 'r') as f:\n",
        "        os.environ['OPENAI_API_KEY'] = f.read().strip()\n",
        "    print(\"API Key de OpenAI cargada.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar la API Key de OpenAI: {e}.\")\n",
        "    raise\n",
        "\n",
        "# Cargar API Key de Pinecone\n",
        "pinecone_api_key_file = '/content/PINECONE_API_KEY.txt'\n",
        "try:\n",
        "    with open(pinecone_api_key_file, 'r') as f:\n",
        "        os.environ['PINECONE_API_KEY'] = f.read().strip()\n",
        "    print(\"API Key de Pinecone cargada.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar la API Key de Pinecone: {e}.\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "S--asZlT11B4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2e7094a-d744-421a-e52e-e01974fa96c2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Key de OpenAI cargada.\n",
            "API Key de Pinecone cargada.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funciones Modulares del Proyecto"
      ],
      "metadata": {
        "id": "GAtZ9diazRpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Valida la carpeta knowledge-base\n",
        "def validar_base_conocimiento():\n",
        "    if not os.path.isdir(Config.KNOWLEDGE_BASE_DIR):\n",
        "        error_message = (\n",
        "            f\"Error: El directorio '{Config.KNOWLEDGE_BASE_DIR}' no fue encontrado.\"\n",
        "        )\n",
        "        raise FileNotFoundError(error_message)\n",
        "    print(f\"Directorio '{Config.KNOWLEDGE_BASE_DIR}' encontrado.\")"
      ],
      "metadata": {
        "id": "dA3qAvh5zeEx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crea o actualiza una base de datos vectorial en Pinecone.\n",
        "\n",
        "\"\"\"\n",
        "1. Carga y divide los documentos.\n",
        "2. Inicializa la conexión con Pinecone.\n",
        "3. Comprueba si el índice ya existe. Si existe, lo elimina para empezar de cero.\n",
        "4. Crea un nuevo índice con la configuración correcta.\n",
        "5. Genera los embeddings y los sube al índice de Pinecone.\n",
        "\"\"\"\n",
        "\n",
        "def crear_base_conocimiento(knowledge_dir: str, index_name: str):\n",
        "    print(\"Cargando y procesando documentos...\")\n",
        "    documents = []\n",
        "    for item_path in glob.glob(f\"{knowledge_dir}/*\"):\n",
        "        if os.path.isdir(item_path):\n",
        "            loader = DirectoryLoader(item_path, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs={'encoding': 'utf-8'}, show_progress=True)\n",
        "            documents.extend(loader.load())\n",
        "        elif os.path.isfile(item_path) and item_path.endswith('.md'):\n",
        "             loader = TextLoader(item_path, encoding='utf-8')\n",
        "             documents.extend(loader.load())\n",
        "\n",
        "    if not documents:\n",
        "        raise ValueError(f\"No se encontraron documentos en el directorio: {knowledge_dir}\")\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=Config.CHUNK_SIZE, chunk_overlap=Config.CHUNK_OVERLAP)\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    print(f\"Total de fragmentos (chunks) creados: {len(chunks)}\")\n",
        "\n",
        "    embeddings = OpenAIEmbeddings(model=Config.EMBEDDING_MODEL_NAME)\n",
        "\n",
        "    # --- Lógica de Pinecone ---\n",
        "\n",
        "    print(f\"Inicializando cliente de Pinecone...\")\n",
        "    pc = PineconeClient()\n",
        "\n",
        "    # Comprobar si el índice ya existe\n",
        "    if index_name in [index.name for index in pc.list_indexes()]:\n",
        "        print(f\"Índice '{index_name}' encontrado.\")\n",
        "        pc.delete_index(index_name)\n",
        "        time.sleep(5)\n",
        "\n",
        "    # Crear un nuevo índice\n",
        "    print(f\"Creando un nuevo índice en Pinecone: '{index_name}'...\")\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=Config.EMBEDDING_DIMENSION,\n",
        "        metric='cosine',\n",
        "        spec=ServerlessSpec(\n",
        "            cloud='aws',\n",
        "            region='us-east-1'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Esperar a que el índice esté listo\n",
        "    while not pc.describe_index(index_name).status['ready']:\n",
        "        print(\"Preparando el índice.\")\n",
        "        time.sleep(1)\n",
        "\n",
        "    print(f\"Añadiendo {len(chunks)} fragmentos al índice de Pinecone.\")\n",
        "\n",
        "    # LangChain genera los embeddings y los carga al índice\n",
        "    vectorstore = Pinecone.from_documents(\n",
        "        documents=chunks,\n",
        "        embedding=embeddings,\n",
        "        index_name=index_name\n",
        "    )\n",
        "\n",
        "    print(f\"\\nBase de datos vectorial en Pinecone creada y poblada.\")\n",
        "    return vectorstore"
      ],
      "metadata": {
        "id": "B5g4slzZzeKE"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Configura la cadena de conversación.\n",
        "def configurar_cadena_conversacional(vectorstore):\n",
        "    llm = ChatOpenAI(model=Config.LLM_MODEL_NAME, temperature=0)\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": Config.SEARCH_K})\n",
        "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=retriever,\n",
        "        memory=memory,\n",
        "    )\n",
        "\n",
        "    print(\"Cadena de conversación configurada.\")\n",
        "    return conversation_chain"
      ],
      "metadata": {
        "id": "TWy3PMZgzeHg"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crea y lanza la interfaz de chat de Gradio.\n",
        "def iniciar_chat_gradio(conversation_chain):\n",
        "    def chat_function_with_sources(question, history):\n",
        "        try:\n",
        "            # 1. Preparar el historial para LangChain\n",
        "            langchain_history = []\n",
        "            for human, ai in history:\n",
        "                if human:\n",
        "                    langchain_history.append({\"role\": \"user\", \"content\": human})\n",
        "                if ai:\n",
        "                    langchain_history.append({\"role\": \"assistant\", \"content\": ai})\n",
        "\n",
        "            # 2. Invocar la cadena de conversación\n",
        "            result = conversation_chain.invoke({\"question\": question, \"chat_history\": langchain_history})\n",
        "            answer = result[\"answer\"]\n",
        "\n",
        "            # 3. Extraer y formatear las fuentes\n",
        "            source_docs = result.get(\"source_documents\", [])\n",
        "            source_text = \"\"\n",
        "            if source_docs:\n",
        "                unique_sources = sorted(list(set(os.path.basename(doc.metadata.get('source', 'Desconocido')) for doc in source_docs)))\n",
        "                source_list = \"\\\\n\".join([f\"- `{source}`\" for source in unique_sources])\n",
        "                source_text = f\"\\\\n\\\\n---\\\\n**Fuentes Consultadas:**\\\\n{source_list}\"\n",
        "\n",
        "            return answer, source_text\n",
        "\n",
        "        except Exception as e:\n",
        "            error_message = f\"Ocurrió un error: {e}\"\n",
        "            return error_message, \"\"\n",
        "\n",
        "    # Construcción de la Interfaz\n",
        "    with gr.Blocks(theme='soft', title=\"Jurid-IA\") as interface:\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            <div style=\"text-align: center;\">\n",
        "                <h1>⚖️ Jurid-IA Asistente Jurídico de IA </h1>\n",
        "                <p>Tu experto entrenado con los documentos internos del Banco XYZ.</p>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        # Área principal del chat y las fuentes\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=4):\n",
        "                chatbot = gr.Chatbot(\n",
        "                    label=\"Chat\",\n",
        "                    height=550\n",
        "                )\n",
        "                sources = gr.Markdown(label=\"Fuentes\", value=\"*Las fuentes consultadas para la última respuesta aparecerán aquí.*\")\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### Ejemplos de Preguntas\")\n",
        "                with gr.Row():\n",
        "                    textbox = gr.Textbox(\n",
        "                        placeholder=\"Escribe tu pregunta aquí...\",\n",
        "                        show_label=False,\n",
        "                        container=False,\n",
        "                        scale=7,\n",
        "                    )\n",
        "                    submit_btn = gr.Button(\"Enviar\", variant=\"primary\", scale=1)\n",
        "\n",
        "                examples = gr.Examples(\n",
        "                    examples=[\n",
        "                        \"¿Cuál es la política de teletrabajo?\",\n",
        "                        \"¿Cuáles son los términos de pago del contrato con 'Tech Innovators'?\",\n",
        "                        \"¿Cuál es el límite para aceptar regalos según el código de conducta?\"\n",
        "                    ],\n",
        "                    inputs=[textbox],\n",
        "                    label=\"Haz clic en un ejemplo para probar\",\n",
        "                    fn=lambda x: x,\n",
        "                    outputs=[textbox]\n",
        "                )\n",
        "\n",
        "\n",
        "        # --- Lógica de Interacción ---\n",
        "\n",
        "        # Función que se ejecuta al enviar el formulario\n",
        "        def handle_submit(question, history):\n",
        "            response, source_text = chat_function_with_sources(question, history)\n",
        "            history.append((question, response))\n",
        "            return \"\", history, source_text\n",
        "\n",
        "        # Conectar el botón de envío\n",
        "        submit_btn.click(\n",
        "            fn=handle_submit,\n",
        "            inputs=[textbox, chatbot],\n",
        "            outputs=[textbox, chatbot, sources],\n",
        "            queue=True\n",
        "        )\n",
        "\n",
        "        # Conectar la acción de presionar \"Enter\"\n",
        "        textbox.submit(\n",
        "            fn=handle_submit,\n",
        "            inputs=[textbox, chatbot],\n",
        "            outputs=[textbox, chatbot, sources],\n",
        "            queue=True\n",
        "        )\n",
        "\n",
        "    # Lanzar la interfaz\n",
        "    interface.launch(debug=True, share=True)"
      ],
      "metadata": {
        "id": "pS0XsCYh28zU"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejecución del Proyecto"
      ],
      "metadata": {
        "id": "oDAqTpaU1jjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función principal que orquesta todo el proceso.\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # 1. Valida que los documentos fuente existan\n",
        "        validar_base_conocimiento()\n",
        "\n",
        "        # 2. Crea la base de datos vectorial en Pinecone\n",
        "        vectorstore = crear_base_conocimiento(\n",
        "            knowledge_dir=Config.KNOWLEDGE_BASE_DIR,\n",
        "            index_name=Config.PINECONE_INDEX_NAME\n",
        "        )\n",
        "\n",
        "        # 3. Configura la cadena de conversación con el vectorstore de Pinecone\n",
        "        conversation_chain = configurar_cadena_conversacional(vectorstore)\n",
        "\n",
        "        # 4. Inicia la interfaz de usuario de Gradio para el chat\n",
        "        iniciar_chat_gradio(conversation_chain)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\\\nHa ocurrido un error en la ejecución: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# Punto de entrada para la ejecución del script\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "upACSF-c100W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        },
        "outputId": "426d3dc8-dc49-448b-9366-26140bc01067"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directorio '/content/knowledge-base' encontrado.\n",
            "Cargando y procesando documentos...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:00<00:00, 6061.13it/s]\n",
            "100%|██████████| 5/5 [00:00<00:00, 8115.91it/s]\n",
            "100%|██████████| 5/5 [00:00<00:00, 8348.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de fragmentos (chunks) creados: 59\n",
            "Inicializando cliente de Pinecone...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Índice 'jurid-ia-idx' encontrado.\n",
            "Creando un nuevo índice en Pinecone: 'jurid-ia-idx'...\n",
            "Añadiendo 59 fragmentos al índice de Pinecone.\n",
            "\n",
            "Base de datos vectorial en Pinecone creada y poblada.\n",
            "Cadena de conversación configurada.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2570773561.py:46: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://9b139fdca222084b47.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9b139fdca222084b47.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://9b139fdca222084b47.gradio.live\n"
          ]
        }
      ]
    }
  ]
}